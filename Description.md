# INTRODUCTION
An significant part of a person's body is their face, which is particularly useful for interpreting
their behaviour and emotional condition. Without realising it, we categorise emotions
frequently. Without uttering a word, the human face is capable of expressing a wide range of
emotions. And unlike certain nonverbal communication techniques, these expressions can
recognized by all types of individuals and are universal. The most significant use of emotion
detection is in facial expression. The topic of emotion recognition is now the subject of much
research. Techniques for detecting mood using facial images yield results quickly and
effectively. Since the period of Aristotle, the ability to recognise emotional state in facial
expression has been a fascinating topic. After 1960, when a catalogue of universal emotions was
created and many systems were suggested, this idea gained greater traction. Our expectations
have increased since the advent of contemporary technology, which has no bounds.
People from all cultures around the globe all use the same facial expressions and body languages 
for joy, sorrow, anger, surprised, anxiety, and contempt. People who watch you can tell how 
you're feeling by the way your muscles move. In addition to many other primates and certain 
other animal species, they are the way by which societal information can be shared between 
humans. Humans may express themselves facially voluntarily or unintentionally. People often
produce involuntary expressions when they are unwell, injured, or uncomfortable. In social 
interaction, facial expressions of emotions play a significant role in understanding others' 
motives. Researcher took notice of that as well since about the capacity to recognise facial 
expressions aids in human-computer interaction, helps advertisers create effective
advertisements, and improves human communication process by modifying emotional 
intelligence (EQ) in people. On a daily basis, people can usually identify emotions by the 
distinctive qualities they present as part of a facial expression. For instance, a smiles or its 
upward movement of the lips' corner are certainly signs which indicate satisfaction. Similar to 
how one emotion can be distinguished from another by additional deformations unique to that
emotion. The questions revolves around the presentation and classification of either dynamic or 
static properties so all the formations and deformations of face pigments are addressed in
research on the automated identification of facial expressions. One of the primary 
informational streams in interpersonal interaction is facial expressions. Because of the
applications across perceptual and cognitive sciences, it is only natural that face emotion
research which had grabbed a lot of attention during the past decade..
With the fast advancement of Artificial Intelligent (AI) capabilities, interest in automated Facial
Emotion Recognition (FER) has indeed been rising recently.

## DIFFERENT METHODS OF CLASSIFICATION
They currently serve a variety of purposes, and human contact with them is growing. Machines
must be given the ability to comprehend the surroundings, particularly human intentions, in 
order to enhance Human Computer Interaction (HCI) and making it much more realistic. 
Employing surveillance cameras and sensor, computers may record the status of their
surroundings. Deep Learning (DL) algorithms have excelled in capturing environmental
conditions in recent years. Since emotions provide information about a person's inner state,
emotion detection is essential for robots to function more effectively. A machine is capable of
identifying human emotions from a series of face photos using DL approaches .
Deep learning models have shown to be particularly helpful for classifying and recognizing
patterns. The characteristics are the most crucial components of any deep learning system. In
this work, we are going to examine the extraction and modification of features for algorithms
like CNN. Algorithms and feature extraction methods from several articles will be compared.
The human facial emotion data may be used as a highly useful example to investigate the nature, 
resilience, and performance of classification methods for various dataset types. Face detection 
methods are often applied to the picture or video frame well before collection of components for 
emotion detection. The following are generalizations of the emotion detection procedures:
1) Preprocessing of data sets
2) Face recognition
3) Extracting feature
4) Classifications
utilizing relevant qualities Throughout this work, we place particular attention on the features
extracting process and emotions recognition utilizing the features extracted.
There have already been a number of projects in this area, thus our objective will not simplybe to create 
a deep learning model with annotated photographs of stable facial emotions and The project's objective 
technically entails training a deep convolutional neural network. Later, this network may be included into 
software that can identify and categorize the seven primary human emotion—happiness, sadness, surprise,
anger, disgust, neutrality, and fear—while increasing its accuracy of given system in comparison to the
baseline accuracy of 14.29 percent. Additionally, the precision of our model's output for each class will 
be examined as part of our study. Future performance of the model is anticipated to include identification 
of wild emotions with more complicated variation in conditions than laboratory conditions images wild
emotions with more complicated variation in conditions than laboratory conditions images.

## PROBLEM STATEMENT
In the evolving landscape of human-computer interaction, understanding and 
responding to human emotions play a crucial role in creating more intuitive and empathetic 
systems. Emotion detection technology aims to enhance user experiences by enabling machines 
to recognize and respond to the emotional states of users. However, there are several challenges 
that need to be addressed to achieve accurate and reliable emotion detection.
This field of face recognition deals with tactics and approaches for reading facial expressions
for emotions. The identification of emotions has become simpler because to several technical
advancements in the area of artificial intelligency and machine learnings. Expressions are
anticipated to become the next communication tool for computers. Automated facial analysis of 
emotions is becoming increasingly necessary. The majority of the studies in this field focuses 
on extracting emotional responses from movies or audio data. The majority of these studies work 
matches and recognises faces, however deep convolutional neural networks havenot been 
utilised to extract emotions from photos. 
Facial Emotion Recognition is a field of study that seeks to extract an emotion from a person's
facial expression. According to the polls, improvements in emotion detection have simplified
complicated systems. There are several uses for FER, which will be covered later. The difficult
problem of emotion recognition arises from the fact that emotions might differ based on the
situation, one's appearance, culture, and facial expression, which produces confusing data.
Understanding facial expression recognition is greatly aided by a survey on the subject.
Approach can be said as a machine learning that uses data models created specifically to do a
certain task. In the fields of image identification, classification, selection, pattern matching, etc.,
deep learning in neural network models has many applications. For selecting features,
picture recognition, and other deep learning applications, multi - modal deep learning is
utilised.

## SOFTWARE REQUIREMENT
Because the program is being created in Python, we used Anaconda for Python 3.6.5 and Spyder. 
Anaconda is an open source version of the Python and R programming languages to be used in 
projects related to data science and machine learnings with the aim of facilitating package 
management and installation . Package versioning is managed by a mechanism called Conda.
The Anaconda package, which provides over than 240+ well-liked data science applications
suitable to Microsoft windows, Linux, and Apple MacOS, is used by more than 60 lakhs
individuals. Spyder is a Python application framework that is open source and cross-platform
for use in the sciences.
SciPy, Matplotlib, Matplotlib, and IPython are all included in Spyder along with additional
open source software. It is provided under the MIT licence. Spyder, which can be extended by
plugins and provides interactive tools for data analysis, includes the Python-specific code quality 
assurance and introspection tools Pyflakes, Pylint, and Rope. With Anaconda, it is crossplatform compatible and usable on a variety of operating systems, including MacPortson the
Mac, WinPython and Python (x,y) on Windows, Debian, Fedora, Gentoo Linux, openSUSE, 
and Ubuntu
The features are:
o compatibility for a number of Python console, especially IPython;
o an editors featuring simple syntax and introspection for writing code;
o a GUI for updating variables. Two plugin which are easily accessible are Pylint's static code
analysis & Code Profiling.
o Conda Package Manager is used.
 Interfaces in hardware
1. Processor: Intel CORE i5 CPU with a minimum speed of 2.9 GHz.
2. RAM: 4 GB or more.
3. Hard drive: 500 GB minimum
 Interfaces in software
1. Google docs
2. Database Storage : Google sheet
3. Operating System : Microsoft Windows10
 PLANNING :
The procedures we used to create this project are as follows:
1. A review of the problem statement.
2. compiling the required information
3. A feasibility assessment of the proposal.
4. Development of an extensive concept
5. Checking the periodicals for past works in this field that are comparable.
6. Selecting an approach for the algorithm's construction.
7. Analyzing the numerous advantages and disadvantages.
8. developing the concept
9. The installation of programs like ANACONDA.
10. Developing an algorithms.
11. An evaluation of the algorithms by a mentor
12. Programming in Python using the generated algorithms The iterative waterfall method was
used to build the system.

## SAMPLE IMAGES
![2](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/720357b1-32b2-4bf2-8741-b3b18f66b484)


![3](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/b5f39f22-fc98-4ea1-8372-a694677756c1)

## FLOWCHART DIAGRAM
A common strategy in computer vision is deep learning. Our model architect's fundamental
construction elements were layers of Convolutional Neural Network . CNNs are known for
mimicking how well the human mind processes images.
A convolutional neural network's fundamental architecture consists of input, some few
convolutional layers, some few dense layers (sometimes referred to as completely layers), and
an output units. These levels are sequentially arranged and stacked in order. This model is
constructed using Sequential() in Keras, and further levels are introduced to build the design.
![FLOWCHART](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/e08aea97-c0ad-42d2-9afe-758fb0c65b3f)
### INPUT LAYER
The photo should always be pre-processed until it can be placed further into layers since the
input layer's parameters are pre-set and predetermined. To identify persons in the picture, we
applied the OpenCV computer vision library. OpenCV's haarcascade frontalface default.xml
rapidly locates and crops the face using adaboost and already learnt filter.
The cropped image would then be reduced to 48 by 48 pixel using cv2.resize and turned to
monochrome using cv2.cvtColor. The three colour components of the initial RGB format are
significantly decreased in size by this procedure (3, 48, 48). The pipeline enables any image to
be accepted as just a (1, 48, 48) linear array by the input nodes.
### CONVOLUTIONAL LAYER
The Convolution 2 dimension layer receives the numpy array, that among them one of it
hyperparameters is the amount of filter. Each filter in the kernel is distinct and uses weights
chosen at random. To generate a feature map, every filters inside the (3 - 3) receptive field
iterates well over original image with shared weight. With the use of convolution, feature maps
are produced that reflect enhanced pixel values like edge and pattern detection. If filter #1 is
evenly distributed to image, a feature map was developed. A set of feature maps are created by
applying extra filters one by one.
![CNN](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/181d84ad-8662-4f8e-b567-5a3fe4641a85)
Dimension reduction method called as pooling is often used over one or maybe more
convolutional layers. It is a crucial step in the building of CNNs since including more
convolutional layer may significantly increase calculation time. we employed a well-liked
pooling technique called MaxPooling2D, which really only keeps the greatest pixel intensity
and employs (2 - 2) frames all over the feature maps. The merged pixels lower the size of an
image by Four pixels.
### DENSE LAYER
![DL](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/5c0692c3-8ac2-42c4-98a3-011a502ece14)
The brain's neurons, which carry impulses across the entire organ, served as the model for the
layers, also known as entirely connected layers. It transforms characteristics utilising layer
coupled by trainable weight with a variety of input characteristics. By first transmitting the
training sets forward as well as subsequently the mistakes backwards, this weights are generated. 
Backpropagation algorithm calculates the weight change required for each level before by first
computing the differences between the actual and real value.
By changing the hyper-parameters, such as training rate and networks densities, researchers may 
regulate its architect's sophistication and learning pace. The networks may progressively adapt
as new information is supplied, reducing errors as it always has.
In theory, the system can receive the signal most accurately more and more layer and node we
put to it. Regardless of the reality that it sounds wonderful, the system also becomes more
capable of approximating the learning set of data. Utilizing dropouts is one way to stop
overfitting and generalizations on uncertain data. During training, Dropout picks a subset of
nodes at random and changes their values to 0. With both the desired level of architect
sophistication, this technique may successfully reduce the model's sensitivity to sound
throughout training.
### OUTPUT LAYER
With the output nodes, we employed softmax rather than the sigmoid function. Each emotions
class' possibility is displayed in this report. The probabilistic components of the different facial
expression may now be accurately represented by the model. Later on, you'll learn it's pointless
to categorise a person facial expressions as only one emotion. Human face expressions are 
typically more nuanced and contain a spectrum of feelings, allowing us to precisely identify a
particular expression.
Deep learning is involved in all this. Despite the challenging pattern of facial movements, a
much more complicated design must be developed in addition to identifying subtle cues.
To even further complication the model, we tested with several combinations of the three
elements listed below:
Employing GPUs computing with g2.2xlarge on Aws, systems with different configurations
were implemented and tested.
This significantly cut down on training process & improved the effectiveness of models
tinkering. The fina net model has 9 convolution layer, with such a max-pooling layer appearing
every 3 convolution levels, as seen in the figure beneath.

![OP](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/fc0c376f-163d-4f67-88d5-04a6faf6e0ff)
# LITERATURE SURVEY
### Article 01:
The article “Face Detection to Recognize Students’ Emotion and Their Engagement: A
SystematicReview” by Benyoussef Abdellaou , A. MOUMEN, Y. EL BOUZEKRI EL IDRISSI & A. Remaida studies 
that have been done on identifying students' moods during a course. Through an exploration and 
analysis ofall the necessary information the authors have gathered for their research, the article 
seeks to address and answer to concerns surrounding face detection. The paper adds to the body of 
knowledge on the subject of the significance of students' emotional states during a course during 
this period of confinement, even though there aren't many articles that directly and in-depth address 
the topic. The following keywords were combined by the authors to search four databases: 
"distance learning," "face detection," "emotion recognition," and "student engagement." They then 
applied filters to the type of articles, retaining only those that were relevant.
### Article 02:
The article titled "A real-time face emotion classification and recognition using deep learning 
model"
by Hussain, Shaik Asif; Salim Abdallah Al Balushi, Ahlam published in the Journal of Physics:
Conference Series provides an overview of the latest research on facial detection and recognition. 
The paper focuses on the use of deep learning models to accurately classify and recognize facial 
emotions in real-time. The authors discuss the significance of real-time face emotion classification 
and recognition invarious fields such as security and camera surveillance. The paper also provides 
details on the network layers, weights, and function values used in the deep learning model. The 
article concludes by discussingthe potential applications of this research in the future and how it 
can be further developed to improve its accuracy and efficiency.
### Article 03:
This article “A Robust Method for Face Recognition and Face Emotion Detection System using 
Support Vector Machines “ by K. M. Rajesh & M. Naveenkumar offers a framework for facial feature and action- based real time face identification and emotion detection systems. When estimating a user's emotions and face features, important facial features are taken into account. Each facial feature's changes are utilized 
to identify the many emotions shown on the face. By training several sets of photos, machine
learning algorithms are utilized to recognize and classify various face emotion classes. OpenCV, 
an open-source computer vision program, and Python machine learning are used to build the 
suggested technique. Additionally, the file offers links to relevant research publications on methods 
for facial expression recognition and face recognition.
### Article 04:
This article “Facial expression analysis with AFFDEX and FACET” by Sabrina Stöckli, Michael 
Schulte- Mecklenbeck, Stefan Borer & Andrea C. Samson
studies the facial expression analysis utilizing the widely used algorithms FACET and AFFDEX .
The study attests to the algorithms' accuracy in identifying different emotions based on facial 
expressions. To evaluate the precision of these algorithms, the authors carried out two 
investigations. Three databases were used in Study 1 to categorize facial expressions that expressed 
emotion. In the second study, participants' facial expressions were recorded as they were shown
emotionally charged images. The outcomes demonstrated how well both algorithms classified
emotions based on facial expressions. In the larger framework of emotion research,the authors did, 
however, also address possible causes of low validity and offer study directions. All things 
considered, this paper offers a helpful tool for scholars who are interested in facial expression
analysis and how it may be used to study emotions.
### Article 05:
The article “Facial Expression Recognition in Adolescents With Mood and Anxiety Disorders” 
by Erin B. McClure, Kayla Pope, Andrea J. Hoberman, Daniel S. Pine, Ellen Leibenluft, looked 
into facial expressionrecognition in teenagers with anxiety and mood disorders is discussed . 
According to the study, young people with bipolar disorder had a higher likelihood of mistaking 
expressions for anger, which could potentially exacerbate social impairment. To determine 
participants' ability to identify emotions, the study used standardized photos of kids and adults with 
high- and low-intensity displays of fear, anger, sadness, or happiness. Additionally, compared to 
the anxiety or comparison groups, the bipolar illness group had greater lifetime incidences of 
attention deficit hyperactivity disorder. The study indicates thatsocial impairment in adolescents 
with mood and anxiety problems may be attributed to misreading of facial expressions, and that
neuroimaging techniques could be used to investigate this idea.
### Article 06:
This article “The Influence of the Activation Function in a Convolution Neural Network Model of 
Facial Expression Recognition”
by Wang, Yingying; Li, Yibin, Song, Yong, Rong & Xuewen (2020) disuss about The 
convolutional neural network (CNN) has been widely used in image recognition field due to its 
goodperformance. This paper proposes a facial expression recognition method based on the CNN
model. According to the design principle of the activation function in CNN model, a new piecewise 
activation function is proposed. Five common activation functions (i.e., sigmoid, tanh, ReLu, leaky 
ReLus and softplus–ReLu, plus the new activation function) have been analysed and compared in 
facial expressionrecognition tasks based on the Keras framework. The Experimental results on two 
public facial expression databases (i.e., JAFFE and FER2013) show that the convolutional neural 
network based on the improved activation function has a better performance than most-of-the-art 
activation functions. The neutral network based on LS–ReLu function can avoid the over-fitting 
problem of the model in the training process and reduce the oscillations problem.
### Article 07:
The article “DETECTION OF FACIAL EXPRESSIONS OF EMOTIONS IN DEPRESSION “ by
Suslow, T.Junghanns, K., & Arolt, V study on the connection between sadness and the ability to detect facial
expressions spatially is discussed in this article. The face-in-the-crowd test was employed in the
study to evaluate depressed people's capacity to distinguish between positive and negative facial 
expressions.The findings demonstrated that those with depression were less able to recognize 
pleasant facial expressions than negative ones. According to the study, depression may have an 
impact on the attentional and perceptual processes necessary for identifying happy emotions.
### Article 08:
The article “Facial Emotion Recognition Using Conventional Machine Learning and Deep 
Learning Methods” by Amjad Rehman Khan covers a variety of methods and algorithms, including as feature
extraction, feature selection, and classification, that are used to identify facial emotions. The 
difficulties in the area are also discussed in the paper, including the requirement for big datasets, 
the impact of variations in culture on facial expressions, and the requirement for real-time 
processing. The author highlights the potential uses of face expression detection in a number of 
industries, such as entertainment, security, and healthcare, in conclusion. All things regarded, one 
interested in the topic offacial expression identification will discover this article to be a helpful
overview of the available research.
### Article 09:
The article “Facial emotion recognition using temporal relational network: an application to Elearning”
by Anil Pise, Hima Vadapalli , Ian Sanders describes how to measure the learning effect and reflect 
on the degree of student involvement in e-learning by using a facial image analysis model based 
on deep learning. The suggested method detects emotional shifts during online learning sessions 
by using a Temporal Relational Network (T RN). In order to look into 3D CNN's capacity for face 
emotion identification, the research reviews various models that are utilized for emotion 
recognition as well as spatiotemporal characteristics. The method of using TRN to extract time 
information based on relationalreasoning is presented, along with the network architecture and data 
pre-processing procedures. The paper's debate on upcoming projects comes to a close.
### Article 10:
This article “Facial expression recognition using lightweight deep learning modeling “
by Mubashir Ahmad , Saira , Omar Alfandi , Asad Masood Khattak & Syed Furqan Qadri Zhejiang 
discuss about the most recent developments in lightweight deep neural network modeling-based 
facial expression recognition. The advantages of adopting lightweight models for facial expression 
identification are covered in the article, along with how they can produce accurate results while 
utilizing little processingpower. In terms of efficiency and accuracy, the suggested approach is 
contrasted with current approaches, and possible real-world uses, including recognizing emotions 
in video calls or security systems, are explored. The article file offers significant perspectives on 
the present status of facial expression recognition technology and its possible uses.
![A](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/e4426cb3-0e22-4851-adac-a2282cd4807c)

![B](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/88f5bd5a-d2f8-4889-8215-d3bd73abb842)

![C](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/cef21df2-1da5-445e-b59c-19050383c948)

## IMPLEMENTATION
To accomplish this task I used 3 computer vision models they are:
• Haarcascade Classifier (for Face extraction)
• Mini Xception (for emotion classification)
 ### Haarcascade :
In their 2001 article "Rapid Object Identification with a Boosted Cascade of Simple Features,"
Paul Viola and Michael Jones proposed an effective object identification method that makes use 
of Haar feature-based cascade classifiers. This machine learning technique uses a huge number 
of both positive and negative pictures to train a cascade function. Utilizing it to locate objects in
other photographs is the next stage.
In this case, face detection will be employed. The approach initially needs a substantial amount 
of both positive (photos of faces) and negative (images of objects) data to train the classifier 
(images without faces). Then we need to extract characteristics from it. This makes use of the
har characteristics from the picture below. They precisely resemble our convolutional kernel.
The total of the pixels beneath the white and black rectangles is subtracted to get a single value
for each feature.

![P](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/e84836bc-1bcb-465b-b916-8b004d52f440)

Now, many characteristics are computed over all viable kernel sizes and locations. ( Just think
about the quantity of math needed. A 24x24 window alone may generate almost 160000 features. 
Each feature computation requires calculating the total number of pixels under the white and 
black rectangles. To deal with this, they created the integrated pictures. No matter how many 
pixels there may be, it simplifies the operation needing more than four to the sum of pixels
computation. Not bad, is it not? Processes are considerably accelerated.
The bulk of the qualities we calculated, however, are not significant. Consider the image below 
as an illustration. Two advantages are shown in the top row. The first attribute picked seems to 
place the most focus on the fact that the area around the eyes is typically darker than the area 
surrounding the nose and cheekbones. The second characteristic was picked because the eyes 
are darker than the bridge of the nose. It doesn't matter if the same windows are used on the
cheeks or any other part of the body, though. Adaboost selects the top attributes.

![Q](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/24e86f10-7962-481f-9970-dc669dae40e7)

We use each and every attribute for this on each of the training photos. In order to classify the
faces as positive or negative, it sets the ideal threshold for each characteristic. However, errors
or improper classification will surely happen. Since they can most effectively discriminate
between photos with and without faces, we select the characteristics with the lowest mistake
rate. (The process is not that simple. Each picture initially has an equal weight. After each
categorization, the weights of the improperly categorised photographs increase. The exact same 
process is repeated. New error rates have been determined. further new weights. Until the proper
accuracy, error rate, or number of features is identified, the process is repeated.
The final classifier is formed by adding these weak classifiers together. It is considered weak
since, by itself, it cannot classify the picture, but when coupled with other classifiers, it can. The 
study found that a 95 percent identification accuracy rate may be achieved using just 200
attributes. They were finally arranged from around 6000 features.
Non-face regions make up the majority of the area of a picture. Therefore, it is preferred to have 
a simple method to detect whether a window is a face region. If not, immediately discard it. 
Please don't think about it again. Focus on the potential location of a face instead. If we do that, 
we'll have more time to look at a suitable facial area. We will have more time to examine a 
potential face region if we do it this way.
### Cascading
Consider that our input image is 640x480 in size. Then, when we move the 2500 features over
the image, each 24 by 24 window must be evaluated. It evaluates whether a threshold exists
before deciding whether the feature is a face by linearly merging all 2500 features.
Instead of using all 2500 features continuously for 24 by 24 times, cascade will be employed.
Out of 2500 features, the first 10 features, the next 20–30 features, and the last 100 features are
classified by three different classifiers. We shall complicate everything in this manner.

![S](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/fea7a857-35fd-4401-b2e1-7536cdfc021f)

We can now distinguish faces in the image. If faces are detected, face positions are returned as
a Rect (x,y,w,h). Once we are aware of their location, we can make a ROI for the face using this 
data and then apply eye detection to it.

![R](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/fca25dfe-238e-411e-918d-9ac5d5e15761)

### Mini Xception

![N](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/5795dca7-4bff-423e-b5b2-2dabc2416538)


The suggested approach for identifying emotions that was based on enhanced deep learning
algorithms was what we had put into practise. A mini-Xception for emotional analysis was
developed using the Xception paradigm. Our suggested model was influenced by the Xception
algorithm. Context separable convolutions and residual modules are used in this construction.
The training data deviates from the original and anticipated feature maps as a result of residual
modules changing the intended projection over two successive layers. As a result, the desired
characteristics H(x) are modified in order to solve the following simplified training problems
F(X):

![G](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/71adc6cd-57f4-4a1b-b97c-bc0ba3fab3c7)

The characteristics of the proposed model are minimized from the Convolution layer using
depth-wise separable convolutions. Depth-wise and point-wise convolutions are the two layers
that make up a depth-wise separable convolution.
Each convolution in the work being given is preceded by a batch normalizing technique and a
ReLUs activation function. The work has four residual depth-wise separated convolution layers. 
To create a prediction, the last layer uses a soft-max activation function and the averageglobal
pool. There are around 60 thousand parameters in this framework.

![M](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/c27e7928-7622-41be-960e-05d5e0c6859c)
Differentiating between spatial and channel cross correlations is the main job of these layers.
They combine the M input channels into N output channels by first applying a DD filter to each 
of the M input channels, followed by N 11M convolution filters. Without taking into
consideration their spatial connections within the channel, 1M convolutions are used to merge
each value in the feature map. Depth-wise separable convolutions reduce computation by 1 N
+ 1 D2 when compared to conventional convolutions. a comparison between a depth-wise
separable convolution layer and a standard convolution layer.

## ALGORITHM :
Step 1: Collecting an image data set is the first step. (In this instance, we're utilising the FER2013 
database, which contains 35887 pre-cropped, 48 by 48 pixel grayscale photos of faces, each of 
which has been assigned a label from one of the seven emotion classes: rage, contempt, fear,
happiness, sorrow, surprise, and neutral.

Step 2: Image pre-processing is step two.

Step 3: Finding a face in each photograph is step three.

Step 4: Images in grayscale are created using the cropped face.

Step 5: The pipeline makes sure that each picture may be entered as a (1, 48, 48) numpy array.

Step 6: The numpy array is sent into the Convolution2D layer in step 5.

Step 7: Convolution produces feature maps in step six.

Step 8: MaxPooling2D, a pooling technique, keeps just the highest pixel value while using (2,
2) windows throughout the feature map.

Step 9: Backward and forward propagation on neural networks were done on the pixel values 
 during training.
 
Step 10: For each emotion class, the Softmax function is presented as a probability.
The model can provide the precise probability composition of the facial expressions ofemotions
 ## CODE
 ```
from statistics import mode
import numpy as np
import cv2
import tensorflow.keras
from tensorflow.keras.models import load_model
from time import sleep
import os
print("Libraries Updated")
from datasets import get_labels
from inference import detect_faces
from inference import draw_text
from inference import draw_bounding_box
from inference import apply_offsets
from inference import load_detection_model
from preprocessor import preprocess_input
print("Libraries Updated")
# parameters for loading data and images
detection_model_path = 'haarcascade_frontalface_default.xml'
emotion_model_path = 'fer2013_mini_XCEPTION.102-0.66.hdf5'
gender_model_path = 'simple_CNN.81-0.96.hdf5'
emotion_labels = get_labels('fer2013')
gender_labels = get_labels('imdb')
font = cv2.FONT_HERSHEY_SIMPLEX
# hyper-parameters for bounding boxes shape
frame_window = 10
gender_offsets = (30, 60)
emotion_offsets = (20, 40)
# loading models
face_detection = load_detection_model(detection_model_path)
emotion_classifier = load_model(emotion_model_path, compile=False)
gender_classifier = load_model(gender_model_path, compile=False)
# getting input model shapes for inference
emotion_target_size = emotion_classifier.input_shape[1:3]
gender_target_size = gender_classifier.input_shape[1:3]
# starting lists for calculating modes
gender_window = []
emotion_window = []
# starting video streaming
cv2.namedWindow('window_frame')
video_capture = cv2.VideoCapture(0)
Emotion_Count = 0
emotion_text = ''
emotion_text_Old = 'check'
Depression_Count = 0
Angry_Count = 0
Sad_Count = 0
Fear_Count = 0
while True:
bgr_image = video_capture.read()[1]
gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)
rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)
faces = detect_faces(face_detection, gray_image)
for face_coordinates in faces:
x1, x2, y1, y2 = apply_offsets(face_coordinates, gender_offsets)
rgb_face = rgb_image[y1:y2, x1:x2]
x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)
gray_face = gray_image[y1:y2, x1:x2]
try:
rgb_face = cv2.resize(rgb_face, (gender_target_size))
gray_face = cv2.resize(gray_face, (emotion_target_size))
except:
continue
gray_face = preprocess_input(gray_face, False)
gray_face = np.expand_dims(gray_face, 0)
gray_face = np.expand_dims(gray_face, -1)
emotion_label_arg = np.argmax(emotion_classifier.predict(gray_face))
emotion_text = emotion_labels[emotion_label_arg]
emotion_window.append(emotion_text)
rgb_face = np.expand_dims(rgb_face, 0)
rgb_face = preprocess_input(rgb_face, False)
gender_prediction = gender_classifier.predict(rgb_face)
gender_label_arg = np.argmax(gender_prediction)
gender_text = gender_labels[gender_label_arg]
gender_window.append(gender_text)
if len(gender_window) > frame_window:
emotion_window.pop(0)
gender_window.pop(0)
try:
emotion_mode = mode(emotion_window)
gender_mode = mode(gender_window)
except:
continue
if gender_text == gender_labels[0]:
color = (0, 0, 255)
else:
color = (255, 0, 0)
draw_bounding_box(face_coordinates, rgb_image, color)
draw_text(face_coordinates, rgb_image, gender_mode,
color, 0, -20, 1, 1)
draw_text(face_coordinates, rgb_image, emotion_mode,
color, 0, -45, 1, 1)
bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)
cv2.imshow('window_frame', bgr_image)
# print(emotion_text)
if( emotion_text_Old == emotion_text ):
# print("Same Emotion")
if( emotion_text == 'angry' ):
Angry_Count = Angry_Count + 1
if( Angry_Count >= 5 ):
print("Angry Emotion Detected")
sleep(3) 
Depression_Flag = 1
elif( emotion_text == 'sad' ):
Sad_Count = Sad_Count + 1
if( Sad_Count >= 5 ):
print("Sad Emotion Detected")
sleep(3) 
Depression_Flag = 1
elif( emotion_text == 'fear' ):
Fear_Count = Fear_Count + 1
if( Fear_Count >= 5 ):
print("Fear Emotion Detected")
sleep(3) 
Depression_Flag = 1
## if( Depression_Flag == 1):
## if( (Sad_Count >= 4) and (Angry_Count >= 1) and (Fear_Count >= 1) 
):
## print("Less Stress or Depression Detected")
## elif( (Sad_Count >= 2) and (Angry_Count >= 4) and (Fear_Count >= 3) 
):
## print("More Stress or Depression Detected")
## elif( (Sad_Count >= 2) and (Angry_Count >= 4) and (Fear_Count >= 4) 
):
## print("High Stress or Depression Detected")
if( Depression_Flag == 1):
if( Sad_Count >= 4 ):
print("Less Stress or Depression Detected")
if( Angry_Count >= 4 ):
print("More Stress or Depression Detected")
if( Fear_Count >= 4 ):
print("Moderate Stress or Depression Detected")
sleep(2)
Angry_Count = 0
Sad_Count = 0
Fear_Count = 0
emotion_text_Old = emotion_text
Depression_Flag = 0
sleep(0.4)
if cv2.waitKey(1) & 0xFF == ord('q'):
break
video_capture.release()
cv2.destroyAllWindows()
print("Project End")
```
### Dataset.Py
```
from scipy.io import loadmat
import pandas as pd
import numpy as np
from random import shuffle
import os
import cv2
class DataManager(object):
"""Class for loading fer2013 emotion classification dataset or
imdb gender classification dataset."""
def __init__(self, dataset_name='imdb',
dataset_path=None, image_size=(48, 48)):
self.dataset_name = dataset_name
self.dataset_path = dataset_path
self.image_size = image_size
if self.dataset_path is not None:
self.dataset_path = dataset_path
elif self.dataset_name == 'imdb':
self.dataset_path = '../datasets/imdb_crop/imdb.mat'
elif self.dataset_name == 'fer2013':
self.dataset_path = '../datasets/fer2013/fer2013.csv'
elif self.dataset_name == 'KDEF':
self.dataset_path = '../datasets/KDEF/'
else:
raise Exception(
'Incorrect dataset name, please input imdb or fer2013')
def get_data(self):
if self.dataset_name == 'imdb':
ground_truth_data = self._load_imdb()
elif self.dataset_name == 'fer2013':
ground_truth_data = self._load_fer2013()
elif self.dataset_name == 'KDEF':
ground_truth_data = self._load_KDEF()
return ground_truth_data
def _load_imdb(self):
face_score_treshold = 3
dataset = loadmat(self.dataset_path)
image_names_array = dataset['imdb']['full_path'][0, 0][0]
gender_classes = dataset['imdb']['gender'][0, 0][0]
face_score = dataset['imdb']['face_score'][0, 0][0]
second_face_score = dataset['imdb']['second_face_score'][0, 0][0]
face_score_mask = face_score > face_score_treshold
second_face_score_mask = np.isnan(second_face_score)
unknown_gender_mask = np.logical_not(np.isnan(gender_classes))
mask = np.logical_and(face_score_mask, second_face_score_mask)
mask = np.logical_and(mask, unknown_gender_mask)
image_names_array = image_names_array[mask]
gender_classes = gender_classes[mask].tolist()
image_names = []
for image_name_arg in range(image_names_array.shape[0]):
image_name = image_names_array[image_name_arg][0]
image_names.append(image_name)
return dict(zip(image_names, gender_classes))
def _load_fer2013(self):
data = pd.read_csv(self.dataset_path)
pixels = data['pixels'].tolist()
width, height = 48, 48
faces = []
for pixel_sequence in pixels:
face = [int(pixel) for pixel in pixel_sequence.split(' ')]
face = np.asarray(face).reshape(width, height)
face = cv2.resize(face.astype('uint8'), self.image_size)
faces.append(face.astype('float32'))
faces = np.asarray(faces)
faces = np.expand_dims(faces, -1)
emotions = pd.get_dummies(data['emotion']).as_matrix()
return faces, emotions
def _load_KDEF(self):
class_to_arg = get_class_to_arg(self.dataset_name)
num_classes = len(class_to_arg)
file_paths = []
for folder, subfolders, filenames in os.walk(self.dataset_path):
for filename in filenames:
if filename.lower().endswith(('.jpg')):
file_paths.append(os.path.join(folder, filename))
num_faces = len(file_paths)
y_size, x_size = self.image_size
faces = np.zeros(shape=(num_faces, y_size, x_size))
emotions = np.zeros(shape=(num_faces, num_classes))
for file_arg, file_path in enumerate(file_paths):
image_array = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)
image_array = cv2.resize(image_array, (y_size, x_size))
faces[file_arg] = image_array
file_basename = os.path.basename(file_path)
file_emotion = file_basename[4:6]
# there are two file names in the dataset
# that don't match the given classes
try:
emotion_arg = class_to_arg[file_emotion]
except:
continue
emotions[file_arg, emotion_arg] = 1
faces = np.expand_dims(faces, -1)
return faces, emotions
def get_labels(dataset_name):
if dataset_name == 'fer2013':
return {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy',
4: 'sad', 5: 'surprise', 6: 'neutral'}
elif dataset_name == 'imdb':
return {0: 'woman', 1: 'man'}
elif dataset_name == 'KDEF':
return {0: 'AN', 1: 'DI', 2: 'AF', 3: 'HA', 4: 'SA', 5: 'SU', 6: 'NE'}
else:
raise Exception('Invalid dataset name')
def get_class_to_arg(dataset_name='fer2013'):
if dataset_name == 'fer2013':
return {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'sad': 4,
'surprise': 5, 'neutral': 6}
elif dataset_name == 'imdb':
return {'woman': 0, 'man': 1}
elif dataset_name == 'KDEF':
return {'AN': 0, 'DI': 1, 'AF': 2, 'HA': 3, 'SA': 4, 'SU': 5, 'NE': 6}
else:
raise Exception('Invalid dataset name')
def split_imdb_data(ground_truth_data, validation_split=.2, do_shuffle=False):
ground_truth_keys = sorted(ground_truth_data.keys())
if do_shuffle is not False:
shuffle(ground_truth_keys)
training_split = 1 - validation_split
num_train = int(training_split * len(ground_truth_keys))
train_keys = ground_truth_keys[:num_train]
validation_keys = ground_truth_keys[num_train:]
return train_keys, validation_keys
def split_data(x, y, validation_split=.2):
num_samples = len(x)
num_train_samples = int((1 - validation_split)*num_samples)
train_x = x[:num_train_samples]
train_y = y[:num_train_samples]
val_x = x[num_train_samples:]
val_y = y[num_train_samples:]
train_data = (train_x, train_y)
val_data = (val_x, val_y)
return train_data, val_data
```
### Inference.Py
```
import cv2
import matplotlib.pyplot as plt
import numpy as np
import tensorflow.keras
from tensorflow.keras.preprocessing import image
def load_image(image_path, grayscale=False, target_size=None):
pil_image = image.load_img(image_path, grayscale, target_size)
return image.img_to_array(pil_image)
def load_detection_model(model_path):
detection_model = cv2.CascadeClassifier(model_path)
return detection_model
def detect_faces(detection_model, gray_image_array):
return detection_model.detectMultiScale(gray_image_array, 1.3, 5)
def draw_bounding_box(face_coordinates, image_array, color):
x, y, w, h = face_coordinates
cv2.rectangle(image_array, (x, y), (x + w, y + h), color, 2)
def apply_offsets(face_coordinates, offsets):
x, y, width, height = face_coordinates
x_off, y_off = offsets
return (x - x_off, x + width + x_off, y - y_off, y + height + y_off)
def draw_text(coordinates, image_array, text, color, x_offset=0, y_offset=0,
font_scale=2, thickness=2):
x, y = coordinates[:2]
cv2.putText(image_array, text, (x + x_offset, y + y_offset),
cv2.FONT_HERSHEY_SIMPLEX,
font_scale, color, thickness, cv2.LINE_AA)
def get_colors(num_classes):
colors = plt.cm.hsv(np.linspace(0, 1, num_classes)).tolist()
colors = np.asarray(colors) * 255
return colors
```
### Preprocessor.Py
```
import numpy as np
#from scipy.misc import imread, imresize
#from scipy.misc.pilutil import imread, imresize
from matplotlib.pyplot import imread
#from scipy.misc import imresize
def preprocess_input(x, v2=True):
x = x.astype('float32')
x = x / 255.0
if v2:
x = x - 0.5
x = x * 2.0
return x
def _imread(image_name):
return imread(image_name)
def _imresize(image_array, size):
return imresize(image_array, size)
def to_categorical(integer_classes, num_classes=2):
integer_classes = np.asarray(integer_classes, dtype='int')
num_samples = integer_classes.shape[0]
categorical = np.zeros((num_samples, num_classes))
categorical[np.arange(num_samples), integer_classes] = 1
return categorica
```
# RESULTS
## Performance of Mini-Xception model
![O](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/a2b5d5bb-9009-4553-8482-3c08c97d66cf)
Here confusion matrix was used to determine precision. The crucial performance parameters,
including accuracy, are looked at for measurable compared with experimental outcomes . These 
variable we be utilise to accomplish the false negative variable used are True Negative, True
Positive, False Negative, & False Positive.
It performed with a 95 % accuracy on test set.Its accuracy was 96.2 %, often on the test set.
The experiment looked into how much sadness there has been in different films. Recognizing
sadness in movies is only a small component of the whole diagnostic process for depression.
Even youngsters who are not really sad right now might develop the condition during the next
several decades.
Therefore, it is important to always keep a tight eye on every one of their extracurriculars. As
a output the video of the very same student shot at other timepoints may be taken into account
in additional investigation. The examination and evaluation of the participant's past and current 
mental health condition, and also the process for determining the degree of depression, may
benefit from it.
## CONCLUSION
Face expression identification system findings from this work endeavor is underpinned by a
reliable face detection and identification model that maps behavioral factors with physiologic
biometrics. Simple geometric elements that have been rebuilt as the identifying system's basic
matching templates are connected to physiological features of the human face with regard to a
variety of expressions including pleasure, sorrow, panic, fury, wonder, and contempt.
From among several emotions expressed we tried to identify from it those for anger, disgust,
anxiety, pleasure, sadness, amaze, and neutral. To deliver the final framework for emotions
detection, several new research have leveraged deep learning model. Even though it might be
challenging, there seems to be a lot to improve when it comes to emotion detection. Existence
of "joy," "neutral," and "anxiety," negative emotions as well as "disgust," "neutral," and "joy,"
positive emotions, was found and studied in depression videos. Facial characteristics were
recognized that used the Cascade classification and small xception classifier using different
training and testing sets. Every video's have positive & negative emotions including its
classification as belonging to the "High Depression," "Moderate Depression," or "Low
Depression" categories were examined. The results were accurately anticipated by the
classification.
The classifier predictions will be even more precise if there are enough training samples. The
training videos were recorded in over a thousand frames, but only the first few were used in the 
assessment. By identifying the video's important frames and extracting them only with the use 
of a key frame extraction method for following work, this procedure can be performed for the 
full video. The current research is limited to the student's most recent video . However, for a
more accurate detection of depression, the student's past should be taken into account.

## FUTURE SCOPE AND APPLICATIONS
Understanding that there can be no specific formula to developing a neural network .
Will provide my maximum efforts. Different tasks will require network topologies aswellas
a lot of trial - and - error to attain the requisite prediction performance.
And because of this Neural networks are sometimes described as "black-box
algorithm" by individuals.
The project's precision were closer to 71 percent, which isn't really poor at all comparing to
other works modelled antecedents. There must be, though, a couple ofareas which we might
improve
● Number and design of convolutional layers
● The number and configuration of thick layer
● drop - out rates
## Applications:
### Analysis of Emotion for campaign testing:
Applying sentiment analytics technologies, it's really feasible to evaluate marketing initiatives
from anywhere from global to local. It ensures that it will be eliciting the desired responses once 
they are introduced to the world. Marketers can identify whether the ads are evoking strong user
engagement and optimistic emotional states using detection and identification software like 
Deep Sight, that also gives marketers the techniques to conduct large-scale experiments on 
various pre defined target demographics to choose the marketing campaign which will have the
greatest influence.
### Emotion analysis for online education:
It is excellent to examine the eLearning trip and make essential improvements using
anonymized emotions recognition for virtual classrooms. Emotional comments from pupils as
they progress throughout every modules in real - time basis may be used to evaluate a college's
lecture notes, instructional approaches, architecture, and design. Find spots of attraction or 
courses tripping blocks and make improvements using genuine face expressions & employee
engagement.
### Emotion recognition in health care:
Healthcare system is one sector that is benefiting from this innovation, with Intelligence face
recognition software determining when patients require therapy, analyze overall emotional
attachment in clinical studies, or assist doctors in choosing the patient satisfaction treatment
strategy.
### Emotion analysis in video game testing:
In order to elicit a certain behavior and mix of feelings out from players, videos games are
developed with such a specified customer in mind. Participants are required to play those
games for a predetermined amount of time during the evaluation process, and all comments
are taken into account when creating the finaloutput. Real-time facial expression detection can 
help identify the feelings that a person is feeling. It is a fantastic supplement for feedback
provided since it offers a more in-depth analysis of the gameplay experience.
### Automotive industry and emotion analysis:
Since car makers all over the world concentrating on producing products more individualized
and secure for people to drive, the automotive sector is also utilizing emotions identification
system. It seems logical that automakers utilize AI to aid them in comprehending human feelings 
as they strive to design smart vehicle functions. Self - driving cars that use face expression 
recognition can warn the driver if they arefeeling sleepy, reducing the number of fatal crashes.

## REFERENCE
[1].B. Abdellaoui, A. MOUMEN, Y. EL BOUZEKRI EL IDRISSI and A. Remaida, "Face 
Detection to RecognizeStudents’ Emotion and Their Engagement: A Systematic Review," 2020 
IEEE 2nd International Conference on Electronics, Control, Optimization and Computer 
Science (ICECOCS), Kenitra, Morocco, 2020, pp. 1-6, doi:10.1109/ICECOCS50124.2020.9314600.

[2]. Hussain, Shaik Asif; Salim Abdallah Al Balushi, Ahlam (2020). A íeal time face emotion
classification and íecognition using deep leaíning model. Jouínal of Physics: Confeíence Seíies,
1432(), 012087–. doi:10.1088/1742-6596/1432/1/012087

[3]. K. M. Rajesh and M. Naveenkumar, "A robust method for face recognition and face emotion
detection system using support vector machines," 2016 International Conference on Electrical,
Electronics, Communication, Computer and Optimization Techniques (ICEECCOT), Mysuru, 
India, 2016,pp. 1-5, doi: 10.1109/ICEECCOT.2016.7955175.

[4].Stöckli, S., Schulte-Mecklenbeck, M., Borer, S. et al. Facial expression analysis with 
AFFDEX and FACET: A validation study. Behav Res 50, 1446–1460 (2018).
https://doi.org/10.3758/s13428- 0170996-1.

[5]. McClure, Erin B.; Pope, Kayla; Hoberman, Andrea J.; Pine, Daniel S.; Leibenluft, Ellen 
(2003). Facial Expression Recognition in Adolescents With Mood and Anxiety Disorders. 
American Journal of Psychiatry, 160(6), 1172–1174. doi:10.1176/appi.ajp.160.6.1172.

[6]. Wang, Yingying; Li, Yibin; Song, Yong; Rong, Xuewen (2020). The Influence of the 
Activation Function in a Convolution Neural Network Model of Facial Expression Recognition. 
Applied Sciences,10(5), 1897–. doi:10.3390/app10051897.

[7]. Suslow, T., Junghanns, K., & Arolt, V. (2001). Detection of Facial Expressions of Emotions 
in Depression. Perceptual and Motor Skills, 92(3), 857-868.
https://doi.org/10.2466/pms.2001.92.3.857

[8]. Khan, Amjad Rehman. 2022. "Facial Emotion Recognition Using Conventional Machine 
Learning and Deep Learning Methods: Current Achievements, Analysis and Remaining 
Challenges" Information 13, no.6: 268. https://doi.org/10.3390/info13060268

[9]. Pise, A., Vadapalli, H. & Sanders, I. Facial emotion recognition using temporal relational 
network: an application to E-learning. Multimed Tools Appl 81, 26633–26653 (2022).
https://doi.org/10.1007/s11042-020-10133-y

[10]. Ahmad, M. et al. (2021). Facial Expression Recognition using Lightweight Deep Learning
Modeling. Mathematical Biosciences and Engineering, 20(5), 8208-8225. doi:10.1000/123456






































