# INTRODUCTION
An significant part of a person's body is their face, which is particularly useful for interpreting
their behaviour and emotional condition. Without realising it, we categorise emotions
frequently. Without uttering a word, the human face is capable of expressing a wide range of
emotions. And unlike certain nonverbal communication techniques, these expressions can
recognized by all types of individuals and are universal. The most significant use of emotion
detection is in facial expression. The topic of emotion recognition is now the subject of much
research. Techniques for detecting mood using facial images yield results quickly and
effectively. Since the period of Aristotle, the ability to recognise emotional state in facial
expression has been a fascinating topic. After 1960, when a catalogue of universal emotions was
created and many systems were suggested, this idea gained greater traction. Our expectations
have increased since the advent of contemporary technology, which has no bounds.
People from all cultures around the globe all use the same facial expressions and body languages 
for joy, sorrow, anger, surprised, anxiety, and contempt. People who watch you can tell how 
you're feeling by the way your muscles move. In addition to many other primates and certain 
other animal species, they are the way by which societal information can be shared between 
humans. Humans may express themselves facially voluntarily or unintentionally. People often
produce involuntary expressions when they are unwell, injured, or uncomfortable. In social 
interaction, facial expressions of emotions play a significant role in understanding others' 
motives. Researcher took notice of that as well since about the capacity to recognise facial 
expressions aids in human-computer interaction, helps advertisers create effective
advertisements, and improves human communication process by modifying emotional 
intelligence (EQ) in people. On a daily basis, people can usually identify emotions by the 
distinctive qualities they present as part of a facial expression. For instance, a smiles or its 
upward movement of the lips' corner are certainly signs which indicate satisfaction. Similar to 
how one emotion can be distinguished from another by additional deformations unique to that
emotion. The questions revolves around the presentation and classification of either dynamic or 
static properties so all the formations and deformations of face pigments are addressed in
research on the automated identification of facial expressions. One of the primary 
informational streams in interpersonal interaction is facial expressions. Because of the
applications across perceptual and cognitive sciences, it is only natural that face emotion
research which had grabbed a lot of attention during the past decade..
With the fast advancement of Artificial Intelligent (AI) capabilities, interest in automated Facial
Emotion Recognition (FER) has indeed been rising recently.

## DIFFERENT METHODS OF CLASSIFICATION
They currently serve a variety of purposes, and human contact with them is growing. Machines
must be given the ability to comprehend the surroundings, particularly human intentions, in 
order to enhance Human Computer Interaction (HCI) and making it much more realistic. 
Employing surveillance cameras and sensor, computers may record the status of their
surroundings. Deep Learning (DL) algorithms have excelled in capturing environmental
conditions in recent years. Since emotions provide information about a person's inner state,
emotion detection is essential for robots to function more effectively. A machine is capable of
identifying human emotions from a series of face photos using DL approaches .
Deep learning models have shown to be particularly helpful for classifying and recognizing
patterns. The characteristics are the most crucial components of any deep learning system. In
this work, we are going to examine the extraction and modification of features for algorithms
like CNN. Algorithms and feature extraction methods from several articles will be compared.
The human facial emotion data may be used as a highly useful example to investigate the nature, 
resilience, and performance of classification methods for various dataset types. Face detection 
methods are often applied to the picture or video frame well before collection of components for 
emotion detection. The following are generalizations of the emotion detection procedures:
1) Preprocessing of data sets
2) Face recognition
3) Extracting feature
4) Classifications
utilizing relevant qualities Throughout this work, we place particular attention on the features
extracting process and emotions recognition utilizing the features extracted.
There have already been a number of projects in this area, thus our objective will not simplybe to create 
a deep learning model with annotated photographs of stable facial emotions and The project's objective 
technically entails training a deep convolutional neural network. Later, this network may be included into 
software that can identify and categorize the seven primary human emotion—happiness, sadness, surprise,
anger, disgust, neutrality, and fear—while increasing its accuracy of given system in comparison to the
baseline accuracy of 14.29 percent. Additionally, the precision of our model's output for each class will 
be examined as part of our study. Future performance of the model is anticipated to include identification 
of wild emotions with more complicated variation in conditions than laboratory conditions images wild
emotions with more complicated variation in conditions than laboratory conditions images.

## PROBLEM STATEMENT
In the evolving landscape of human-computer interaction, understanding and 
responding to human emotions play a crucial role in creating more intuitive and empathetic 
systems. Emotion detection technology aims to enhance user experiences by enabling machines 
to recognize and respond to the emotional states of users. However, there are several challenges 
that need to be addressed to achieve accurate and reliable emotion detection.
This field of face recognition deals with tactics and approaches for reading facial expressions
for emotions. The identification of emotions has become simpler because to several technical
advancements in the area of artificial intelligency and machine learnings. Expressions are
anticipated to become the next communication tool for computers. Automated facial analysis of 
emotions is becoming increasingly necessary. The majority of the studies in this field focuses 
on extracting emotional responses from movies or audio data. The majority of these studies work 
matches and recognises faces, however deep convolutional neural networks havenot been 
utilised to extract emotions from photos. 
Facial Emotion Recognition is a field of study that seeks to extract an emotion from a person's
facial expression. According to the polls, improvements in emotion detection have simplified
complicated systems. There are several uses for FER, which will be covered later. The difficult
problem of emotion recognition arises from the fact that emotions might differ based on the
situation, one's appearance, culture, and facial expression, which produces confusing data.
Understanding facial expression recognition is greatly aided by a survey on the subject.
Approach can be said as a machine learning that uses data models created specifically to do a
certain task. In the fields of image identification, classification, selection, pattern matching, etc.,
deep learning in neural network models has many applications. For selecting features,
picture recognition, and other deep learning applications, multi - modal deep learning is
utilised.

## SOFTWARE REQUIREMENT
Because the program is being created in Python, we used Anaconda for Python 3.6.5 and Spyder. 
Anaconda is an open source version of the Python and R programming languages to be used in 
projects related to data science and machine learnings with the aim of facilitating package 
management and installation . Package versioning is managed by a mechanism called Conda.
The Anaconda package, which provides over than 240+ well-liked data science applications
suitable to Microsoft windows, Linux, and Apple MacOS, is used by more than 60 lakhs
individuals. Spyder is a Python application framework that is open source and cross-platform
for use in the sciences.
SciPy, Matplotlib, Matplotlib, and IPython are all included in Spyder along with additional
open source software. It is provided under the MIT licence. Spyder, which can be extended by
plugins and provides interactive tools for data analysis, includes the Python-specific code quality 
assurance and introspection tools Pyflakes, Pylint, and Rope. With Anaconda, it is crossplatform compatible and usable on a variety of operating systems, including MacPortson the
Mac, WinPython and Python (x,y) on Windows, Debian, Fedora, Gentoo Linux, openSUSE, 
and Ubuntu
The features are:
o compatibility for a number of Python console, especially IPython;
o an editors featuring simple syntax and introspection for writing code;
o a GUI for updating variables. Two plugin which are easily accessible are Pylint's static code
analysis & Code Profiling.
o Conda Package Manager is used.
 Interfaces in hardware
1. Processor: Intel CORE i5 CPU with a minimum speed of 2.9 GHz.
2. RAM: 4 GB or more.
3. Hard drive: 500 GB minimum
 Interfaces in software
1. Google docs
2. Database Storage : Google sheet
3. Operating System : Microsoft Windows10
 PLANNING :
The procedures we used to create this project are as follows:
1. A review of the problem statement.
2. compiling the required information
3. A feasibility assessment of the proposal.
4. Development of an extensive concept
5. Checking the periodicals for past works in this field that are comparable.
6. Selecting an approach for the algorithm's construction.
7. Analyzing the numerous advantages and disadvantages.
8. developing the concept
9. The installation of programs like ANACONDA.
10. Developing an algorithms.
11. An evaluation of the algorithms by a mentor
12. Programming in Python using the generated algorithms The iterative waterfall method was
used to build the system.

## SAMPLE IMAGES
![2](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/720357b1-32b2-4bf2-8741-b3b18f66b484)


![3](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/b5f39f22-fc98-4ea1-8372-a694677756c1)

## FLOWCHART DIAGRAM
A common strategy in computer vision is deep learning. Our model architect's fundamental
construction elements were layers of Convolutional Neural Network . CNNs are known for
mimicking how well the human mind processes images.
A convolutional neural network's fundamental architecture consists of input, some few
convolutional layers, some few dense layers (sometimes referred to as completely layers), and
an output units. These levels are sequentially arranged and stacked in order. This model is
constructed using Sequential() in Keras, and further levels are introduced to build the design.
![FLOWCHART](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/e08aea97-c0ad-42d2-9afe-758fb0c65b3f)
### INPUT LAYER
The photo should always be pre-processed until it can be placed further into layers since the
input layer's parameters are pre-set and predetermined. To identify persons in the picture, we
applied the OpenCV computer vision library. OpenCV's haarcascade frontalface default.xml
rapidly locates and crops the face using adaboost and already learnt filter.
The cropped image would then be reduced to 48 by 48 pixel using cv2.resize and turned to
monochrome using cv2.cvtColor. The three colour components of the initial RGB format are
significantly decreased in size by this procedure (3, 48, 48). The pipeline enables any image to
be accepted as just a (1, 48, 48) linear array by the input nodes.
### CONVOLUTIONAL LAYER
The Convolution 2 dimension layer receives the numpy array, that among them one of it
hyperparameters is the amount of filter. Each filter in the kernel is distinct and uses weights
chosen at random. To generate a feature map, every filters inside the (3 - 3) receptive field
iterates well over original image with shared weight. With the use of convolution, feature maps
are produced that reflect enhanced pixel values like edge and pattern detection. If filter #1 is
evenly distributed to image, a feature map was developed. A set of feature maps are created by
applying extra filters one by one.
![CNN](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/181d84ad-8662-4f8e-b567-5a3fe4641a85)
Dimension reduction method called as pooling is often used over one or maybe more
convolutional layers. It is a crucial step in the building of CNNs since including more
convolutional layer may significantly increase calculation time. we employed a well-liked
pooling technique called MaxPooling2D, which really only keeps the greatest pixel intensity
and employs (2 - 2) frames all over the feature maps. The merged pixels lower the size of an
image by Four pixels.
### DENSE LAYER
![DL](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/5c0692c3-8ac2-42c4-98a3-011a502ece14)
The brain's neurons, which carry impulses across the entire organ, served as the model for the
layers, also known as entirely connected layers. It transforms characteristics utilising layer
coupled by trainable weight with a variety of input characteristics. By first transmitting the
training sets forward as well as subsequently the mistakes backwards, this weights are generated. 
Backpropagation algorithm calculates the weight change required for each level before by first
computing the differences between the actual and real value.
By changing the hyper-parameters, such as training rate and networks densities, researchers may 
regulate its architect's sophistication and learning pace. The networks may progressively adapt
as new information is supplied, reducing errors as it always has.
In theory, the system can receive the signal most accurately more and more layer and node we
put to it. Regardless of the reality that it sounds wonderful, the system also becomes more
capable of approximating the learning set of data. Utilizing dropouts is one way to stop
overfitting and generalizations on uncertain data. During training, Dropout picks a subset of
nodes at random and changes their values to 0. With both the desired level of architect
sophistication, this technique may successfully reduce the model's sensitivity to sound
throughout training.
### OUTPUT LAYER
With the output nodes, we employed softmax rather than the sigmoid function. Each emotions
class' possibility is displayed in this report. The probabilistic components of the different facial
expression may now be accurately represented by the model. Later on, you'll learn it's pointless
to categorise a person facial expressions as only one emotion. Human face expressions are 
typically more nuanced and contain a spectrum of feelings, allowing us to precisely identify a
particular expression.
Deep learning is involved in all this. Despite the challenging pattern of facial movements, a
much more complicated design must be developed in addition to identifying subtle cues.
To even further complication the model, we tested with several combinations of the three
elements listed below:
Employing GPUs computing with g2.2xlarge on Aws, systems with different configurations
were implemented and tested.
This significantly cut down on training process & improved the effectiveness of models
tinkering. The fina net model has 9 convolution layer, with such a max-pooling layer appearing
every 3 convolution levels, as seen in the figure beneath.

![OP](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/fc0c376f-163d-4f67-88d5-04a6faf6e0ff)
# LITERATURE SURVEY
### Article 01:
The article “Face Detection to Recognize Students’ Emotion and Their Engagement: A
SystematicReview” by Benyoussef Abdellaou , A. MOUMEN, Y. EL BOUZEKRI EL IDRISSI & A. Remaida studies 
that have been done on identifying students' moods during a course. Through an exploration and 
analysis ofall the necessary information the authors have gathered for their research, the article 
seeks to address and answer to concerns surrounding face detection. The paper adds to the body of 
knowledge on the subject of the significance of students' emotional states during a course during 
this period of confinement, even though there aren't many articles that directly and in-depth address 
the topic. The following keywords were combined by the authors to search four databases: 
"distance learning," "face detection," "emotion recognition," and "student engagement." They then 
applied filters to the type of articles, retaining only those that were relevant.
### Article 02:
The article titled "A real-time face emotion classification and recognition using deep learning 
model"
by Hussain, Shaik Asif; Salim Abdallah Al Balushi, Ahlam published in the Journal of Physics:
Conference Series provides an overview of the latest research on facial detection and recognition. 
The paper focuses on the use of deep learning models to accurately classify and recognize facial 
emotions in real-time. The authors discuss the significance of real-time face emotion classification 
and recognition invarious fields such as security and camera surveillance. The paper also provides 
details on the network layers, weights, and function values used in the deep learning model. The 
article concludes by discussingthe potential applications of this research in the future and how it 
can be further developed to improve its accuracy and efficiency.
### Article 03:
This article “A Robust Method for Face Recognition and Face Emotion Detection System using 
Support Vector Machines “ by K. M. Rajesh & M. Naveenkumar offers a framework for facial feature and action- based real time face identification and emotion detection systems. When estimating a user's emotions and face features, important facial features are taken into account. Each facial feature's changes are utilized 
to identify the many emotions shown on the face. By training several sets of photos, machine
learning algorithms are utilized to recognize and classify various face emotion classes. OpenCV, 
an open-source computer vision program, and Python machine learning are used to build the 
suggested technique. Additionally, the file offers links to relevant research publications on methods 
for facial expression recognition and face recognition.
### Article 04:
This article “Facial expression analysis with AFFDEX and FACET” by Sabrina Stöckli, Michael 
Schulte- Mecklenbeck, Stefan Borer & Andrea C. Samson
studies the facial expression analysis utilizing the widely used algorithms FACET and AFFDEX .
The study attests to the algorithms' accuracy in identifying different emotions based on facial 
expressions. To evaluate the precision of these algorithms, the authors carried out two 
investigations. Three databases were used in Study 1 to categorize facial expressions that expressed 
emotion. In the second study, participants' facial expressions were recorded as they were shown
emotionally charged images. The outcomes demonstrated how well both algorithms classified
emotions based on facial expressions. In the larger framework of emotion research,the authors did, 
however, also address possible causes of low validity and offer study directions. All things 
considered, this paper offers a helpful tool for scholars who are interested in facial expression
analysis and how it may be used to study emotions.
### Article 05:
The article “Facial Expression Recognition in Adolescents With Mood and Anxiety Disorders” 
by Erin B. McClure, Kayla Pope, Andrea J. Hoberman, Daniel S. Pine, Ellen Leibenluft, looked 
into facial expressionrecognition in teenagers with anxiety and mood disorders is discussed . 
According to the study, young people with bipolar disorder had a higher likelihood of mistaking 
expressions for anger, which could potentially exacerbate social impairment. To determine 
participants' ability to identify emotions, the study used standardized photos of kids and adults with 
high- and low-intensity displays of fear, anger, sadness, or happiness. Additionally, compared to 
the anxiety or comparison groups, the bipolar illness group had greater lifetime incidences of 
attention deficit hyperactivity disorder. The study indicates thatsocial impairment in adolescents 
with mood and anxiety problems may be attributed to misreading of facial expressions, and that
neuroimaging techniques could be used to investigate this idea.
### Article 06:
This article “The Influence of the Activation Function in a Convolution Neural Network Model of 
Facial Expression Recognition”
by Wang, Yingying; Li, Yibin, Song, Yong, Rong & Xuewen (2020) disuss about The 
convolutional neural network (CNN) has been widely used in image recognition field due to its 
goodperformance. This paper proposes a facial expression recognition method based on the CNN
model. According to the design principle of the activation function in CNN model, a new piecewise 
activation function is proposed. Five common activation functions (i.e., sigmoid, tanh, ReLu, leaky 
ReLus and softplus–ReLu, plus the new activation function) have been analysed and compared in 
facial expressionrecognition tasks based on the Keras framework. The Experimental results on two 
public facial expression databases (i.e., JAFFE and FER2013) show that the convolutional neural 
network based on the improved activation function has a better performance than most-of-the-art 
activation functions. The neutral network based on LS–ReLu function can avoid the over-fitting 
problem of the model in the training process and reduce the oscillations problem.
### Article 07:
The article “DETECTION OF FACIAL EXPRESSIONS OF EMOTIONS IN DEPRESSION “ by
Suslow, T.Junghanns, K., & Arolt, V study on the connection between sadness and the ability to detect facial
expressions spatially is discussed in this article. The face-in-the-crowd test was employed in the
study to evaluate depressed people's capacity to distinguish between positive and negative facial 
expressions.The findings demonstrated that those with depression were less able to recognize 
pleasant facial expressions than negative ones. According to the study, depression may have an 
impact on the attentional and perceptual processes necessary for identifying happy emotions.
### Article 08:
The article “Facial Emotion Recognition Using Conventional Machine Learning and Deep 
Learning Methods” by Amjad Rehman Khan covers a variety of methods and algorithms, including as feature
extraction, feature selection, and classification, that are used to identify facial emotions. The 
difficulties in the area are also discussed in the paper, including the requirement for big datasets, 
the impact of variations in culture on facial expressions, and the requirement for real-time 
processing. The author highlights the potential uses of face expression detection in a number of 
industries, such as entertainment, security, and healthcare, in conclusion. All things regarded, one 
interested in the topic offacial expression identification will discover this article to be a helpful
overview of the available research.
### Article 09:
The article “Facial emotion recognition using temporal relational network: an application to Elearning”
by Anil Pise, Hima Vadapalli , Ian Sanders describes how to measure the learning effect and reflect 
on the degree of student involvement in e-learning by using a facial image analysis model based 
on deep learning. The suggested method detects emotional shifts during online learning sessions 
by using a Temporal Relational Network (T RN). In order to look into 3D CNN's capacity for face 
emotion identification, the research reviews various models that are utilized for emotion 
recognition as well as spatiotemporal characteristics. The method of using TRN to extract time 
information based on relationalreasoning is presented, along with the network architecture and data 
pre-processing procedures. The paper's debate on upcoming projects comes to a close.
### Article 10:
This article “Facial expression recognition using lightweight deep learning modeling “
by Mubashir Ahmad , Saira , Omar Alfandi , Asad Masood Khattak & Syed Furqan Qadri Zhejiang 
discuss about the most recent developments in lightweight deep neural network modeling-based 
facial expression recognition. The advantages of adopting lightweight models for facial expression 
identification are covered in the article, along with how they can produce accurate results while 
utilizing little processingpower. In terms of efficiency and accuracy, the suggested approach is 
contrasted with current approaches, and possible real-world uses, including recognizing emotions 
in video calls or security systems, are explored. The article file offers significant perspectives on 
the present status of facial expression recognition technology and its possible uses.
![A](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/e4426cb3-0e22-4851-adac-a2282cd4807c)

![B](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/88f5bd5a-d2f8-4889-8215-d3bd73abb842)

![C](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/cef21df2-1da5-445e-b59c-19050383c948)

## IMPLEMENTATION
To accomplish this task I used 3 computer vision models they are:
• Haarcascade Classifier (for Face extraction)
• Mini Xception (for emotion classification)
 ### Haarcascade :
In their 2001 article "Rapid Object Identification with a Boosted Cascade of Simple Features,"
Paul Viola and Michael Jones proposed an effective object identification method that makes use 
of Haar feature-based cascade classifiers. This machine learning technique uses a huge number 
of both positive and negative pictures to train a cascade function. Utilizing it to locate objects in
other photographs is the next stage.
In this case, face detection will be employed. The approach initially needs a substantial amount 
of both positive (photos of faces) and negative (images of objects) data to train the classifier 
(images without faces). Then we need to extract characteristics from it. This makes use of the
har characteristics from the picture below. They precisely resemble our convolutional kernel.
The total of the pixels beneath the white and black rectangles is subtracted to get a single value
for each feature.

![P](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/e84836bc-1bcb-465b-b916-8b004d52f440)

Now, many characteristics are computed over all viable kernel sizes and locations. ( Just think
about the quantity of math needed. A 24x24 window alone may generate almost 160000 features. 
Each feature computation requires calculating the total number of pixels under the white and 
black rectangles. To deal with this, they created the integrated pictures. No matter how many 
pixels there may be, it simplifies the operation needing more than four to the sum of pixels
computation. Not bad, is it not? Processes are considerably accelerated.
The bulk of the qualities we calculated, however, are not significant. Consider the image below 
as an illustration. Two advantages are shown in the top row. The first attribute picked seems to 
place the most focus on the fact that the area around the eyes is typically darker than the area 
surrounding the nose and cheekbones. The second characteristic was picked because the eyes 
are darker than the bridge of the nose. It doesn't matter if the same windows are used on the
cheeks or any other part of the body, though. Adaboost selects the top attributes.

![Q](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/24e86f10-7962-481f-9970-dc669dae40e7)

We use each and every attribute for this on each of the training photos. In order to classify the
faces as positive or negative, it sets the ideal threshold for each characteristic. However, errors
or improper classification will surely happen. Since they can most effectively discriminate
between photos with and without faces, we select the characteristics with the lowest mistake
rate. (The process is not that simple. Each picture initially has an equal weight. After each
categorization, the weights of the improperly categorised photographs increase. The exact same 
process is repeated. New error rates have been determined. further new weights. Until the proper
accuracy, error rate, or number of features is identified, the process is repeated.
The final classifier is formed by adding these weak classifiers together. It is considered weak
since, by itself, it cannot classify the picture, but when coupled with other classifiers, it can. The 
study found that a 95 percent identification accuracy rate may be achieved using just 200
attributes. They were finally arranged from around 6000 features.
Non-face regions make up the majority of the area of a picture. Therefore, it is preferred to have 
a simple method to detect whether a window is a face region. If not, immediately discard it. 
Please don't think about it again. Focus on the potential location of a face instead. If we do that, 
we'll have more time to look at a suitable facial area. We will have more time to examine a 
potential face region if we do it this way.
### Cascading
Consider that our input image is 640x480 in size. Then, when we move the 2500 features over
the image, each 24 by 24 window must be evaluated. It evaluates whether a threshold exists
before deciding whether the feature is a face by linearly merging all 2500 features.
Instead of using all 2500 features continuously for 24 by 24 times, cascade will be employed.
Out of 2500 features, the first 10 features, the next 20–30 features, and the last 100 features are
classified by three different classifiers. We shall complicate everything in this manner.

![S](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/fea7a857-35fd-4401-b2e1-7536cdfc021f)

We can now distinguish faces in the image. If faces are detected, face positions are returned as
a Rect (x,y,w,h). Once we are aware of their location, we can make a ROI for the face using this 
data and then apply eye detection to it.

![R](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/fca25dfe-238e-411e-918d-9ac5d5e15761)

### Mini Xception

![N](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/5795dca7-4bff-423e-b5b2-2dabc2416538)


The suggested approach for identifying emotions that was based on enhanced deep learning
algorithms was what we had put into practise. A mini-Xception for emotional analysis was
developed using the Xception paradigm. Our suggested model was influenced by the Xception
algorithm. Context separable convolutions and residual modules are used in this construction.
The training data deviates from the original and anticipated feature maps as a result of residual
modules changing the intended projection over two successive layers. As a result, the desired
characteristics H(x) are modified in order to solve the following simplified training problems
F(X):

![G](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/71adc6cd-57f4-4a1b-b97c-bc0ba3fab3c7)

The characteristics of the proposed model are minimized from the Convolution layer using
depth-wise separable convolutions. Depth-wise and point-wise convolutions are the two layers
that make up a depth-wise separable convolution.
Each convolution in the work being given is preceded by a batch normalizing technique and a
ReLUs activation function. The work has four residual depth-wise separated convolution layers. 
To create a prediction, the last layer uses a soft-max activation function and the averageglobal
pool. There are around 60 thousand parameters in this framework.

![M](https://github.com/RuchithaReddy28/Emotion-Detection-Mini-Project/assets/93427261/c27e7928-7622-41be-960e-05d5e0c6859c)
Differentiating between spatial and channel cross correlations is the main job of these layers.
They combine the M input channels into N output channels by first applying a DD filter to each 
of the M input channels, followed by N 11M convolution filters. Without taking into
consideration their spatial connections within the channel, 1M convolutions are used to merge
each value in the feature map. Depth-wise separable convolutions reduce computation by 1 N
+ 1 D2 when compared to conventional convolutions. a comparison between a depth-wise
separable convolution layer and a standard convolution layer.

## ALGORITHM :
Step 1: Collecting an image data set is the first step. (In this instance, we're utilising the FER2013 
database, which contains 35887 pre-cropped, 48 by 48 pixel grayscale photos of faces, each of 
which has been assigned a label from one of the seven emotion classes: rage, contempt, fear,
happiness, sorrow, surprise, and neutral.

Step 2: Image pre-processing is step two.

Step 3: Finding a face in each photograph is step three.

Step 4: Images in grayscale are created using the cropped face.

Step 5: The pipeline makes sure that each picture may be entered as a (1, 48, 48) numpy array.

Step 6: The numpy array is sent into the Convolution2D layer in step 5.

Step 7: Convolution produces feature maps in step six.

Step 8: MaxPooling2D, a pooling technique, keeps just the highest pixel value while using (2,
2) windows throughout the feature map.

Step 9: Backward and forward propagation on neural networks were done on the pixel values 
 during training.
 
Step 10: For each emotion class, the Softmax function is presented as a probability.
The model can provide the precise probability composition of the facial expressions ofemotions

























